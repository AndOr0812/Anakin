#---------------------------------------------------------------
##  configuration file of external model convert to paddle inference model
##---------------------------------------------------------------
#
##---------------------------------------------------------------
##  Default configuration options
##  @Param Framework:
##         The model you need to parser. e.g.:
##         CAFFE, PADDLE, [ TENSORFLOW, MXNET, TORCH ]  ...
##         Now we support CAFFE, PADDLE
##         next we will support MXNET, TENSORFLOW ...
##  @Param SavePath:
##         The path where converted model will be saved
##  @Param ResultName
##         Name in paddle inference model of parsed model.
##  @Param LaunchBoard: (OFF/ON)
##         Whether to launch paddle inference graph dash board ( you must set the right ip and port at the same time if LaunchBoard is ON)
##  @Param ip
##         paddle inference graph dash board server ip ( local boardcast ip or real ip)
##  @Param port
##         paddle inference graph dash board server port ( you need to set os open the port )
##  @Param OptimizedGraph:
##             |- enable (OFF/ON) : Whether to visualize the necessary compute and optimization analysis of graph
##             `- path: This place the optimized paddle inference model path generated by paddle inference framework's api graph::save
##  @Param LogToPath
##         Path the log saved.
##  @Param WithColor: (OFF/ON)
##         Wether to usecolorful log
##
##  @Param TARGET::CAFFE ...
##         You only need to fill in the framework config
##         you need to convert
##  @Param ProtoPaths:
##         Protobuf define files, maybe a list.
##  @Param PrototxtPath:
##         Json define prototxt file path  of you model
##  @Param ModelPath:
##         Path of you binary model.
##  @Param DEBUG:
##         NET:
##         LoadPaths:
##         SavePath:
##         SaveFormat: text
##
##--------------------------------------------------------------
#
OPTIONS:
    Framework: FLUID
    SavePath: ./output
    ResultName: googlenet
    Config:
        LaunchBoard: ON
        Server:
            ip: 0.0.0.0
            port: 8888
        OptimizedGraph:
            enable: OFF
            path: /path/to/paddle_inference_model_optimized/googlenet.paddle_inference_model.bin.saved
    LOGGER:
        LogToPath: ./log/
        WithColor: ON 

TARGET:
    FLUID:
        # path of fluid inference model
        Debug: NULL                            # Generally no need to modify.
        ModelPath: /path/to/your/model/        # The upper path of a fluid inference model.
        NetType:                               # Generally no need to modify.
